{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "dataset_link = \"[https://drive.google.com/drive/folders/1NEs0rpFelfzSWAJ6y832EDpW9ImQH4QJ]\"\n"
      ],
      "metadata": {
        "id": "F2D82k-8vyyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "Wj4mlpaXcKJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[1.]Where to collect the Data?**\n",
        "\n",
        "1.Kaggle\n",
        "\n",
        "2.UCI Machine Learning Repository\n",
        "\n",
        "3.Google Dataset Search"
      ],
      "metadata": {
        "id": "V3PvgA4Pv51V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "QE15-cLBcNgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[2.]Importing Dataset through kaggle API**"
      ],
      "metadata": {
        "id": "MxzP6Ocu4lPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the Kaggle libreary\n",
        "!pip install kaggle\n",
        "\n",
        "#configuring the path of Kaggle.json file\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "#API to fetch the dataset from kaggle\n",
        "!kaggle competitions download -c LANL-Earthquake-Prediction\n",
        "\n",
        "#extracting the compressed Dataset\n",
        "from zipfile import ZipFile\n",
        "dataset='/content/LANL-Earthquake-Prediction.zip'\n",
        "with ZipFile(dataset,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('The dataset is extracted')"
      ],
      "metadata": {
        "id": "wM43BSmlmEPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "6gABraRoceZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[3.]Handling Missing Values in Machine Learning**"
      ],
      "metadata": {
        "id": "oMMELJleynRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods to Handle Missing Values:\n",
        "\n",
        "1.Imputation\n",
        "\n",
        "2.Dropping"
      ],
      "metadata": {
        "id": "xXtGKwBtyyzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with missing values\n",
        "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],\n",
        "        'Math': [85, 92, None, 78, 88],\n",
        "        'Science': [90, None, 88, 75, 85]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Handling missing values by filling them with the mean\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa8ke4FMoWv9",
        "outputId": "7a0d3c24-009a-4658-d5a2-305d5fc89382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Name   Math  Science\n",
            "0    Alice  85.00     90.0\n",
            "1      Bob  92.00     84.5\n",
            "2  Charlie  85.75     88.0\n",
            "3    David  78.00     75.0\n",
            "4    Emily  88.00     85.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-a8368df7dc1e>:10: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  df.fillna(df.mean(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with missing values\n",
        "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],\n",
        "        'Math': [85, 92, None, 78, 88],\n",
        "        'Science': [90, None, 88, 75, 85]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Drop rows with any missing values\n",
        "df.dropna(inplace=True)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn7zFB80pA4H",
        "outputId": "0fef47f4-3f36-450a-edd6-c5f2a2e7f25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Name  Math  Science\n",
            "0  Alice  85.0     90.0\n",
            "3  David  78.0     75.0\n",
            "4  Emily  88.0     85.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "234stjabVTv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[4.] Data Standardization**"
      ],
      "metadata": {
        "id": "oqDsQvD5N983"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of standardizing the data to a common format and common range"
      ],
      "metadata": {
        "id": "Q7HsDWpDON68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with the data\n",
        "data = {\n",
        "    'Age': [30, 45, 20],\n",
        "    'Income': [50, 80, 30]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Convert the scaled data back to a DataFrame\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(df)\n",
        "print(\"\\nScaled Data:\")\n",
        "print(scaled_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psPLxgB6i2Rp",
        "outputId": "f9f9070c-2842-4d9a-d2cd-6a9475ccfb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "   Age  Income\n",
            "0   30      50\n",
            "1   45      80\n",
            "2   20      30\n",
            "\n",
            "Scaled Data:\n",
            "        Age    Income\n",
            "0 -0.162221 -0.162221\n",
            "1  1.297771  1.297771\n",
            "2 -1.135550 -1.135550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "0VIAR3s2f6sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[5.]Label Encoding**"
      ],
      "metadata": {
        "id": "fuRlJpXfehFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting the labels into numeric form"
      ],
      "metadata": {
        "id": "yFbqnMPpenAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a list of animal labels\n",
        "animal_labels = ['cat', 'dog', 'rabbit', 'cat', 'dog', 'rabbit']\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_labels = label_encoder.fit_transform(animal_labels)\n",
        "\n",
        "# Print the encoded labels\n",
        "print(\"Encoded Labels:\", encoded_labels)\n",
        "\n",
        "# Print the mapping of original labels to encoded labels\n",
        "print(\"Label Mapping:\", dict(zip(animal_labels, encoded_labels)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9vyXwN4nkrd",
        "outputId": "ca962fbd-a4ff-4b49-f2a4-1f7b18c5f6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Labels: [0 1 2 0 1 2]\n",
            "Label Mapping: {'cat': 0, 'dog': 1, 'rabbit': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "****"
      ],
      "metadata": {
        "id": "FPqKNV8ipS5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[6.]Train_Test_Split**"
      ],
      "metadata": {
        "id": "5VgTFYB_6bTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Generate sample data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "X = np.random.rand(100, 3)  # Features\n",
        "y = np.random.rand(100)  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the sizes of the training and testing sets\n",
        "print(\"Total set size:\",len(X))\n",
        "print(\"Training set size:\", len(X_train))\n",
        "print(\"Testing set size:\", len(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKLFDoGypwtR",
        "outputId": "c15bcd50-49f8-4592-fb02-e5b4b87ebe65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total set size: 100\n",
            "Training set size: 80\n",
            "Testing set size: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "fUtLEt03qxr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[7.]Imbalaced Dataset**"
      ],
      "metadata": {
        "id": "GVFCU8EQdr-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load a dataset from sklearn.datasets\n",
        "data = fetch_openml(name='credit-g', version=1)\n",
        "credit_card_data = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "credit_card_data['Class'] = data.target\n",
        "\n",
        "# Distribution of the two classes\n",
        "print(\"Original class distribution:\")\n",
        "print(credit_card_data['Class'].value_counts())\n",
        "\n",
        "# Separating the legit and fraudulent transactions\n",
        "legit = credit_card_data[credit_card_data.Class == 'good']\n",
        "fraud = credit_card_data[credit_card_data.Class == 'bad']\n",
        "\n",
        "# Shape of legit and fraud transactions\n",
        "print(\"Original shape of legit and fraud transactions:\")\n",
        "print(legit.shape)\n",
        "print(fraud.shape)\n",
        "\n",
        "# Under-Sampling\n",
        "legit_sample = legit.sample(n=300)\n",
        "new_dataset = pd.concat([legit_sample, fraud], axis=0)\n",
        "print(\"Class distribution after under-sampling:\")\n",
        "print(new_dataset['Class'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG-OrGHNh3J8",
        "outputId": "588c6bc4-feeb-4f9b-bbca-ca91e79f2fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original class distribution:\n",
            "good    700\n",
            "bad     300\n",
            "Name: Class, dtype: int64\n",
            "Original shape of legit and fraud transactions:\n",
            "(700, 21)\n",
            "(300, 21)\n",
            "Class distribution after under-sampling:\n",
            "bad     300\n",
            "good    300\n",
            "Name: Class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "LBKEbZLCYAfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[8.]Feature Extraction Of Text Data:Tf-idf Vectorizer**"
      ],
      "metadata": {
        "id": "JR-24ptNFZSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the 20 newsgroups dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "# Display the categories\n",
        "print(\"Categories:\", newsgroups_train.target_names)\n",
        "\n",
        "# Display an example text from the dataset\n",
        "print(\"\\nExample Text:\")\n",
        "print(newsgroups_train.data[0])\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the text data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Print the shape of the transformed data (number of documents, size of vocabulary)\n",
        "print(\"\\nShape of transformed data:\", X_tfidf.shape)\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the first 10 features (words in the vocabulary)\n",
        "print(\"\\nFirst 10 features:\")\n",
        "print(feature_names[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThHQ0LPugi86",
        "outputId": "ef63a1b2-9687-4f06-bf27-638c16e81cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "\n",
            "Example Text:\n",
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Shape of transformed data: (11314, 130107)\n",
            "\n",
            "First 10 features:\n",
            "['00' '000' '0000' '00000' '000000' '00000000' '0000000004' '0000000005'\n",
            " '00000000b' '00000001']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "waqLoaDcjXxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[9.]Numerical Dataset Processing**"
      ],
      "metadata": {
        "id": "4HuNim4ZUze8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Sample numerical dataset\n",
        "data = {\n",
        "    'Age': [25, 30, np.nan, 35, 40],\n",
        "    'Income': [50000, 60000, 75000, np.nan, 90000],\n",
        "    'Score': [4.5, 5.2, 3.9, 4.1, np.nan]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define numerical features\n",
        "numerical_features = ['Age', 'Income', 'Score']\n",
        "\n",
        "# Create a pipeline for numerical preprocessing\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
        "    ('scaler', StandardScaler())  # Standardize features by removing the mean and scaling to unit variance\n",
        "])\n",
        "\n",
        "# Apply numerical preprocessing to numerical features\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numerical_pipeline, numerical_features)\n",
        "])\n",
        "\n",
        "# Fit and transform the data\n",
        "preprocessed_data = preprocessor.fit_transform(df)\n",
        "\n",
        "# Convert preprocessed data back to a DataFrame for visualization\n",
        "preprocessed_df = pd.DataFrame(preprocessed_data, columns=numerical_features)\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(df)\n",
        "print(\"\\nPreprocessed Data:\")\n",
        "print(preprocessed_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHxAl4AZMpI0",
        "outputId": "2ab54127-19ab-47fd-8708-9a7d9abf2f43"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "    Age   Income  Score\n",
            "0  25.0  50000.0    4.5\n",
            "1  30.0  60000.0    5.2\n",
            "2   NaN  75000.0    3.9\n",
            "3  35.0      NaN    4.1\n",
            "4  40.0  90000.0    NaN\n",
            "\n",
            "Preprocessed Data:\n",
            "   Age    Income     Score\n",
            "0 -1.5 -1.383208  0.168763\n",
            "1 -0.5 -0.645497  1.743886\n",
            "2  0.0  0.461069 -1.181342\n",
            "3  0.5  0.000000 -0.731307\n",
            "4  1.5  1.567636  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***"
      ],
      "metadata": {
        "id": "oeoZVy5dNWM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[10.]Textual Data Processing**"
      ],
      "metadata": {
        "id": "NhcDuVwlNbhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-AhYb8hOQRt",
        "outputId": "43787820-3e22-47d6-db0e-183a5f6db70a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "data = {\n",
        "    'text': [\"I love chatting with ChatGPT!\",\n",
        "             \"Natural Language Processing is amazing.\",\n",
        "             \"Preprocessing text data can be tricky, but it's essential for ML.\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    # Join tokens back into text\n",
        "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply text preprocessing to the 'text' column\n",
        "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Convert preprocessed text into TF-IDF vectors\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['preprocessed_text'])\n",
        "\n",
        "# Convert TF-IDF matrix into DataFrame for visualization\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(df[['text']])\n",
        "print(\"\\nPreprocessed Data:\")\n",
        "print(df[['preprocessed_text']])\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_df)\n"
      ],
      "metadata": {
        "id": "feFiogi6JMfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f211467-431b-4ff4-c0e7-18d5e2b7f249"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "                                                text\n",
            "0                      I love chatting with ChatGPT!\n",
            "1            Natural Language Processing is amazing.\n",
            "2  Preprocessing text data can be tricky, but it'...\n",
            "\n",
            "Preprocessed Data:\n",
            "                             preprocessed_text\n",
            "0                        love chatting chatgpt\n",
            "1          natural language processing amazing\n",
            "2  preprocessing text data tricky essential ml\n",
            "\n",
            "TF-IDF Matrix:\n",
            "   amazing  chatgpt  chatting      data  essential  language     love  \\\n",
            "0      0.0  0.57735   0.57735  0.000000   0.000000       0.0  0.57735   \n",
            "1      0.5  0.00000   0.00000  0.000000   0.000000       0.5  0.00000   \n",
            "2      0.0  0.00000   0.00000  0.408248   0.408248       0.0  0.00000   \n",
            "\n",
            "         ml  natural  preprocessing  processing      text    tricky  \n",
            "0  0.000000      0.0       0.000000         0.0  0.000000  0.000000  \n",
            "1  0.000000      0.5       0.000000         0.5  0.000000  0.000000  \n",
            "2  0.408248      0.0       0.408248         0.0  0.408248  0.408248  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wn4G_hOqOLpB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}